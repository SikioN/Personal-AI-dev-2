Сценарий Выступления: Deep Dive в Обучение и Temporal Quadruplets

=========================================
ЧАСТЬ 1: АНАТОМИЯ ЭТАПА ОБУЧЕНИЯ (MEMORIZE PIPELINE)
=========================================
"Коллеги, ключевая ценность нашего проекта — это не RAG, а процесс **построения знаний (Knowledge Construction)**. 
Давайте разберем "под капотом" Pipeline Обучения (`MemorizePipeline`), который превращает сырой текст в базу знаний.

Этот процесс состоит из трех критических фаз:

1.  **Семантическая Экстракция (LLMExtractor)**
    В отличие от регулярных выражений, мы используем Reasoning-способности LLM.
    На вход подается текст, а на выходе мы требуем строгий JSON.
    Мы не просто ищем "слова", мы выделяем **Сущности (Nodes)** и, что самое важное, **Отношения (Relations)**.
    *Пример:* Из фразы "Глава Сбера Герман Греф выступил на AI Journey" система извлекает триплет:
    `(Герман Греф) -> [ДОЛЖНОСТЬ: Глава] -> (Сбер)` и `(Герман Греф) -> [ДЕЙСТВИЕ: Выступил] -> (AI Journey)`.

2.  **Верификация Знаний (Conflict Resolution)**
    Это наш "иммунитет" против галлюцинаций. Перед записью факта `LLMUpdator` делает запрос в граф: "Что мы уже знаем про Германа Грефа?".
    Если граф говорит "Он CEO Яндекса" (старый факт), а новый текст говорит "Он CEO Сбера", LLM решает: **это обновление или противоречие?**
    Только так
     мы поддерживаем граф в актуальном состоянии (Consistency).

3.  **Гибридная Запись (Hybrid Storage)**
    Текстовые описания уходят в VectorDB (Chroma) для нечеткого поиска, а структурные связи — в Neo4j для точных запросов."

=========================================
ЧАСТЬ 2: ВНЕДРЕНИЕ 4-Й МЕТКИ (TEMPORAL QUADRUPLETS)
=========================================
"Текущая проблема графов — они статичны. Мир меняется во времени, а триплеты `(S, R, O)` "замораживают" состояние навсегда.
Мы переходим к архитектуре **Квадруплетов (S, R, O, T)**.

Вот как мы технически реализуем внедрение Времени на всех слоях:

**УРОВЕНЬ 1: Ingestion (Промпт-инжиниринг)**
Мы модифицируем `LLMExtractor`.
Сейчас LLM ищет только факты. Мы добавляем в System Prompt инструкцию **Temporal Tagging**:
> "Extract entities AND the specific time context (absolute or relative) implies in the text."
LLM должна нормализовать время: "в прошлом году" -> `2024`, "в октябре" -> `2024-10`.
Результат парсинга меняется с `(Fact)` на `(Fact, Time_Scope)`.

**УРОВЕНЬ 2: Application (Дата-Классы)**
В файле `data_structs.py` мы трансформируем класс `Triplet`.
Вместо того чтобы плодить сложные узлы, мы добавляем атрибут `valid_at` (timestamp) прямо в класс `Relation`.
**Техническая суть:** Факт перестает быть "абсолютной истиной", он становится "истиной в моменте времени T".

**УРОВЕНЬ 3: Storage (Neo4j Edge Properties)**
Это самое важное изменение архитектуры.
Мы отказываемся от создания отдельных узлов `(Time Node: 2024)`, которые создают "спагетти-связи" в графе.
Вместо этого мы используем нативные возможности Neo4j: **Properties on Relationships**.
Cypher-запрос на вставку меняется:
`MATCH (s), (o) CREATE (s)-[:WORKED_AT { since: '2020', until: '2022' }]->(o)`

**Результат:**
Это позволяет нам делать **Temporal Logic Queries**:
"Кто был директором *до* 2021 года?" -> Граф фильтрует ребра по свойству `since < 2021` за миллисекунды.
Мы получаем систему, которая понимает причинно-следственные связи и хронологию событий."

=========================================
ЧАСТЬ 3: ИНФЕРЕНС И RAG (QA PIPELINE)
=========================================
"Теперь о том, как мы используем эти знания во время ответа пользователю (`QAPipeline`). 
Наш Инференс — это не просто поиск похожих кусков текста. Это **Graph-Augmented Generation**.

1.  **Гибридный Поиск (Hybrid Retrieval)**
    Когда поступает запрос "Где работал Греф до Сбера?", мы запускаем поиск в двух плоскостях:
    *   **Vector Search (ChromaDB):** Находит семантически близкие узлы (anchor nodes).
    *   **Graph Traversal (Neo4j):** От найденных узлов мы "ходим" по связям на 1-2 шага вглубь (1-2 hops). Это позволяет подтянуть контекст, который *не содержал* слов из запроса, но *связан* с ними логически.

2.  **Формирование Контекста**
    Мы собираем все найденные триплеты в локальный подграф. Это и есть "Knowledge Snapshot" для текущего вопроса.

3.  **Synthesis (LLM Reasoning)**
    Финальный этап: мы скармливаем этот структурированный подграф в LLM вместе с вопросом.
    
    **Как здесь поможет Temporal Quadruplets?**
    Сейчас мы отдаем LLM "кучу фактов".
    С внедрением времени мы сможем делать **Pre-Filtering**:
    Если вопрос про "прошлый год", мы на уровне Cypher-запроса отсекаем связи, не попадающие в интервал.
    LLM получит чистый, хронологически верный контекст, что радикально снизит количество ошибок в ответах на временные вопросы."
