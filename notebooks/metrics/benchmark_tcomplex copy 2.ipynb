{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c4cde2",
   "metadata": {},
   "source": [
    "# TComplex + Finetuned E5 Benchmark\n",
    "\n",
    "This notebook benchmarks three configurations on the CronKGQA test set:\n",
    "1. **Baseline + Finetuning**: E5-Small (Finetuned) retrieval + re-ranking (No Temporal Logic).\n",
    "2. **TComplex + Baseline + Finetuning**: E5-Small (Finetuned) + Temporal Logic (Filtering/Sorting) + TComplex Re-ranking.\n",
    "3. **TComplex (Pure)**: Original CronKGQA model (DistilBERT + TComplex scoring over all entities), identifying the Time or Entity directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85e2220f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed directory to project root: /Users/nmuravya/Desktop/KG_sber/Personal-AI-dev 2\n",
      "Added /Users/nmuravya/Desktop/KG_sber/Personal-AI-dev 2 to sys.path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nmuravya/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.30.2 at schema.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/nmuravya/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.30.2 at common.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/nmuravya/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.30.2 at milvus.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/nmuravya/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.30.2 at rg.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/nmuravya/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.30.2 at feder.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/nmuravya/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.30.2 at msg.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Используется Apple Silicon GPU (MPS)\n",
      "✓ Используется Apple Silicon GPU (MPS)\n",
      "✓ Используется Apple Silicon GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Robustly set project root\n",
    "def set_project_root():\n",
    "    current_dir = os.getcwd()\n",
    "    if 'notebooks' in current_dir:\n",
    "        while 'src' not in os.listdir(current_dir):\n",
    "            parent = os.path.dirname(current_dir)\n",
    "            if parent == current_dir:\n",
    "                break\n",
    "            current_dir = parent\n",
    "        os.chdir(current_dir)\n",
    "        print(f\"Changed directory to project root: {current_dir}\")\n",
    "    \n",
    "    if current_dir not in sys.path:\n",
    "        sys.path.append(current_dir)\n",
    "        print(f\"Added {current_dir} to sys.path\")\n",
    "    \n",
    "    # Add CronKGQA to path for imports\n",
    "    cron_path = os.path.join(current_dir, 'CronKGQA', 'CronKGQA')\n",
    "    if cron_path not in sys.path:\n",
    "        sys.path.append(cron_path)\n",
    "\n",
    "set_project_root()\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "from src.kg_model.knowledge_graph_model import KnowledgeGraphModel, KnowledgeGraphModelConfig\n",
    "from src.db_drivers.vector_driver.embedders import EmbedderModelConfig\n",
    "from src.kg_model.embeddings_model import EmbeddingsModelConfig\n",
    "from src.db_drivers.vector_driver import VectorDriverConfig, VectorDBConnectionConfig\n",
    "from src.kg_model.graph_model import GraphModelConfig\n",
    "from src.db_drivers.graph_driver import GraphDriverConfig\n",
    "from src.utils.data_structs import TripletCreator\n",
    "from src.utils.kg_navigator import KGNavigator\n",
    "\n",
    "# Force PyTorch\n",
    "os.environ['USE_TF'] = '0'\n",
    "from transformers import DistilBertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9c08263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Standalone QA Engine to avoid Import Hell ---\n",
    "class StandaloneQAEngine:\n",
    "    def __init__(self, kg_model, finetuned_model_path):\n",
    "        self.kg_model = kg_model\n",
    "        \n",
    "        # Init Embedder\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        if os.path.exists(finetuned_model_path):\n",
    "            model_name = finetuned_model_path\n",
    "        else:\n",
    "            model_name = \"intfloat/multilingual-e5-small\"\n",
    "            \n",
    "        print(f\"Loading S-BERT: {model_name}\")\n",
    "        self.encoder = SentenceTransformer(model_name)\n",
    "        \n",
    "        # Load Mapper\n",
    "        from src.utils.wikidata_utils import WikidataMapper\n",
    "        kg_data_path = \"wikidata_big/kg\"\n",
    "        self.mapper = WikidataMapper(kg_data_path)\n",
    "        \n",
    "        # Init Temporal Scorer (TComplex KGE)\n",
    "        self.temporal_scorer = None\n",
    "        self.ent2id = {}\n",
    "        self.id2ent = {}\n",
    "        try:\n",
    "            from src.kg_model.temporal.temporal_model import TemporalScorer\n",
    "            self.temporal_scorer = TemporalScorer(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            self.ent2id = self.temporal_scorer.ent2id\n",
    "            self.id2ent = {v: k for k, v in self.ent2id.items()}\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not initialize TemporalScorer: {e}\")\n",
    "\n",
    "        # Init Pure CronKGQA Model\n",
    "        self.pure_qa_model = None\n",
    "        self.tokenizer = None\n",
    "        self.init_pure_model()\n",
    "\n",
    "        # Extractor placeholder\n",
    "        self.extractor_override = None \n",
    "\n",
    "    def init_pure_model(self):\n",
    "        try:\n",
    "            print(\"Loading Pure CronKGQA Model...\")\n",
    "            from qa_models import QA_model_EmbedKGQA\n",
    "            class Args:\n",
    "                lm_frozen = 1\n",
    "                frozen = 1\n",
    "            \n",
    "            if self.temporal_scorer:\n",
    "                tkbc_model = self.temporal_scorer.model\n",
    "                qa_model = QA_model_EmbedKGQA(tkbc_model, Args())\n",
    "                \n",
    "                ckpt_path = \"models/cronkgqa/qa_model.ckpt\"\n",
    "                if os.path.exists(ckpt_path):\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "                    # qa_model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "                    # Note: Strict=False because sometimes minor key differences exist\n",
    "                    state_dict = torch.load(ckpt_path, map_location=device)\n",
    "                    qa_model.load_state_dict(state_dict, strict=False)\n",
    "                    qa_model.to(device)\n",
    "                    qa_model.eval()\n",
    "                    self.pure_qa_model = qa_model\n",
    "                    \n",
    "                    # Filter out useless keys and remap if necessary\n",
    "                    self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "                    print(\"Pure CronKGQA Model Loaded Successfully.\")\n",
    "                else:\n",
    "                    print(f\"Warning: QA Model checkpoint not found at {ckpt_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load Pure CronKGQA Model: {e}\")\n",
    "\n",
    "    def get_ranked_results(self, query: str, top_k: int = 5):\n",
    "        # 1. Extract entities (Mocked)\n",
    "        if self.extractor_override:\n",
    "            extraction, _ = self.extractor_override.perform(query)\n",
    "            if isinstance(extraction, list):\n",
    "                item = extraction[0]\n",
    "            else:\n",
    "                item = extraction\n",
    "            entities = item.get('entities', [])\n",
    "            query_time = item.get('time')\n",
    "        else:\n",
    "            entities = [query]\n",
    "            query_time = None\n",
    "            \n",
    "        # 2. Match nodes via Mapper (Strict ID lookup)\n",
    "        mapped_ids = []\n",
    "        for ent in entities:\n",
    "            wd_id = self.mapper.get_id(ent)\n",
    "            if wd_id:\n",
    "                mapped_ids.append(wd_id)\n",
    "        \n",
    "        all_matched_nodes = []\n",
    "        connector = self.kg_model.graph_struct.db_conn\n",
    "        for mid in mapped_ids:\n",
    "            if hasattr(connector, 'strid_nodes_index'):\n",
    "                internal_ids = connector.strid_nodes_index.get(mid, [])\n",
    "                for iid in internal_ids:\n",
    "                    if iid in connector.nodes:\n",
    "                        all_matched_nodes.append(connector.nodes[iid])\n",
    "\n",
    "        if not all_matched_nodes:\n",
    "            return []\n",
    "\n",
    "        # 3. Retrieve Neighborhood\n",
    "        node_ids = [n.id for n in all_matched_nodes]\n",
    "        nav = KGNavigator(self.kg_model)\n",
    "        candidate_triplets = nav.get_neighborhood(node_ids, depth=1)\n",
    "        \n",
    "        if not candidate_triplets:\n",
    "            return []\n",
    "\n",
    "        # Deduplicate\n",
    "        seen = set()\n",
    "        unique_candidates = []\n",
    "        for t in candidate_triplets:\n",
    "            if t.id not in seen:\n",
    "                unique_candidates.append(t)\n",
    "                seen.add(t.id)\n",
    "\n",
    "        # 4. Rank\n",
    "        query_emb = self.encoder.encode([query])[0]\n",
    "        \n",
    "        triplets_text = []\n",
    "        for t in unique_candidates:\n",
    "             _, text = TripletCreator.stringify(t)\n",
    "             triplets_text.append(text)\n",
    "             \n",
    "        triplet_embs = self.encoder.encode(triplets_text)\n",
    "        \n",
    "        results = []\n",
    "        def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        for t, text, emb in zip(unique_candidates, triplets_text, triplet_embs):\n",
    "            score = np.dot(query_emb, emb) / (np.linalg.norm(query_emb) * np.linalg.norm(emb))\n",
    "            e5_conf = float(max(0, score))\n",
    "            final_conf = e5_conf\n",
    "            \n",
    "            if query_time and self.temporal_scorer and self.temporal_scorer.model:\n",
    "                s_qid = t.start_node.prop.get('wd_id')\n",
    "                r_pid = t.relation.prop.get('wd_id')\n",
    "                o_qid = t.end_node.prop.get('wd_id')\n",
    "                if s_qid and r_pid and o_qid:\n",
    "                    try:\n",
    "                        logit = self.temporal_scorer.score(s_qid, r_pid, o_qid, query_time)\n",
    "                        if logit > -10.0:\n",
    "                            final_conf = (e5_conf * 0.7) + (sigmoid(logit) * 0.3)\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "            \n",
    "            results.append({'triplet': t, 'confidence': final_conf})\n",
    "            \n",
    "        results.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "        return results[:top_k]\n",
    "    \n",
    "    def get_pure_tcomplex_rank(self, question, head_qids, time_qid=None, top_k=10):\n",
    "        if not self.pure_qa_model or not self.tokenizer:\n",
    "            return []\n",
    "        \n",
    "        # Prepare Inputs\n",
    "        device = next(self.pure_qa_model.parameters()).device\n",
    "        \n",
    "        # Tokenize Question\n",
    "        tokenized_q = self.tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        q_ids = tokenized_q['input_ids'].to(device)\n",
    "        q_mask = tokenized_q['attention_mask'].to(device)\n",
    "        \n",
    "        # Prepare Entities (Heads)\n",
    "        # Map QIDs to KGE Indices\n",
    "        head_indices = []\n",
    "        for qid in head_qids:\n",
    "            if qid in self.ent2id:\n",
    "                head_indices.append(self.ent2id[qid])\n",
    "        \n",
    "        if not head_indices:\n",
    "            return []\n",
    "            \n",
    "        # Pick first valid head\n",
    "        head_tensor = torch.LongTensor([head_indices[0]]).to(device)\n",
    "        \n",
    "        # Dummy Tail/Time tensors (required by forward)\n",
    "        tail_tensor = torch.LongTensor([0]).to(device)\n",
    "        time_tensor = torch.LongTensor([0]).to(device)\n",
    "        \n",
    "        # Forward pass: (q, mask, heads, tails, times)\n",
    "        # This should execute forward logic and return scores over ONLY entities/times\n",
    "        # BUT QA_model_EmbedKGQA forward signature expects heads, tails, times\n",
    "        # Let's inspect qa_models.py forward again\n",
    "        # It returns scores = torch.cat((scores_entity, scores_time), dim=1)\n",
    "        # scores_entity uses head, relation (from q), time\n",
    "        # scores_time uses head, relation, tail\n",
    "        \n",
    "        # If we pass dummy tail/time, the specific score_entity/score_time output for THAT tail/time will be junk\n",
    "        # BUT Wait. forward returns scores vs ALL entities?\n",
    "        # Look at score_entity method:\n",
    "        # (lhs * rel) @ all_entities.t()\n",
    "        # It IGNORES the 'tail' argument passed in!! (The tail arg is only for 'rhs' which is used if NOT combine_all?)\n",
    "        # No. 'rhs' is computed from tail_embedding.\n",
    "        # score_entity combines head and tail? \n",
    "        # If combine_all_entities_bool is True (default usually in CronKGQA for Eval), it ignores tail.\n",
    "        # Let's try passing dummies.\n",
    "        \n",
    "        batch = (q_ids, q_mask, head_tensor, tail_tensor, time_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            scores = self.pure_qa_model(batch)\n",
    "            # scores shape: (1, num_entities + num_times)\n",
    "        \n",
    "        # Get Top K\n",
    "        try:\n",
    "             top_scores, top_indices = torch.topk(scores, k=top_k)\n",
    "             top_indices = top_indices.cpu().numpy()[0]\n",
    "             results = []\n",
    "             for idx in top_indices:\n",
    "                 if idx in self.id2ent:\n",
    "                     label = self.id2ent[idx]\n",
    "                     results.append(label) \n",
    "             return results\n",
    "        except:\n",
    "             return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f25ecded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing QA Engine...\n",
      "✓ Используется Apple Silicon GPU (MPS)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464da7aed62941209df2672a0b78dd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: intfloat/multilingual-e5-small\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hydrating graph from wikidata_big/kg/full.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing triplets: 100%|██████████| 328635/328635 [00:05<00:00, 59105.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hydration complete. Nodes: 122569, Triplets: 328635\n",
      "Loading S-BERT: intfloat/multilingual-e5-small\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de5a7c4341d4d7bb527e217ca46d56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: intfloat/multilingual-e5-small\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TemporalScorer resources from wikidata_big/kg/tkbc_processed_data/wikidata_big/...\n",
      "Loaded mappings: 125726 entities, 203 relations, 9621 timestamps\n",
      "Loading weights from models/cronkgqa/tcomplex.ckpt...\n",
      "TemporalScorer initialized successfully.\n",
      "Warning: Could not initialize TemporalScorer: 'TemporalScorer' object has no attribute 'ent2id'\n",
      "Loading Pure CronKGQA Model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48f73e372f347d58d87b8f079c278c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0f931db90d4a04a89041d10b956c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1b7e183f104556bb45f2abf09d8758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DistilBertModel LOAD REPORT from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing LM params\n",
      "Failed to load Pure CronKGQA Model: 'Args' object has no attribute 'combine_all_ents'\n",
      "Engine Ready.\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing QA Engine...\")\n",
    "\n",
    "# 1. Initialize QA Engine\n",
    "g_driver_conf = GraphDriverConfig(db_vendor='inmemory_graph')\n",
    "g_model_conf = GraphModelConfig(driver_config=g_driver_conf)\n",
    "\n",
    "model_path = \"models/finetuned_e5\"\n",
    "if not os.path.exists(model_path):\n",
    "    model_path = \"intfloat/multilingual-e5-small\"\n",
    "    \n",
    "emb_conf = EmbedderModelConfig(model_name_or_path=model_path)\n",
    "\n",
    "nodes_path = \"data/graph_structures/vectorized_nodes/wikidata_test\"\n",
    "triplets_path = \"data/graph_structures/vectorized_triplets/wikidata_test\"\n",
    "\n",
    "nodes_cfg = VectorDriverConfig(\n",
    "    db_vendor='chroma', db_config=VectorDBConnectionConfig(\n",
    "        conn={'path': nodes_path},\n",
    "        db_info={'db': 'default_db', 'table': \"personalaitable\"}))\n",
    "        \n",
    "triplets_cfg = VectorDriverConfig(\n",
    "    db_vendor='chroma', db_config=VectorDBConnectionConfig(\n",
    "        conn={'path': triplets_path},\n",
    "        db_info={'db': 'default_db', 'table': \"personalaitable\"}))\n",
    "\n",
    "embs_conf = EmbeddingsModelConfig(\n",
    "    nodesdb_driver_config=nodes_cfg,\n",
    "    tripletsdb_driver_config=triplets_cfg,\n",
    "    embedder_config=emb_conf\n",
    ")\n",
    "\n",
    "kg_conf = KnowledgeGraphModelConfig(graph_config=g_model_conf, embeddings_config=embs_conf)\n",
    "kg_model = KnowledgeGraphModel(config=kg_conf)\n",
    "\n",
    "# Hydrate Graph\n",
    "from src.utils.wikidata_utils import WikidataMapper\n",
    "from src.utils.graph_loader import hydrate_in_memory_graph\n",
    "kg_data_path = \"wikidata_big/kg\"\n",
    "mapper = WikidataMapper(kg_data_path)\n",
    "hydrate_in_memory_graph(kg_model, mapper, kg_data_path)\n",
    "\n",
    "# Init Engine\n",
    "engine = StandaloneQAEngine(kg_model, model_path)\n",
    "print(\"Engine Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec3b6644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test Data\n",
    "test_path = 'wikidata_big/questions/test.pickle'\n",
    "with open(test_path, 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "# Helper classes\n",
    "class MockExtractor:\n",
    "    def __init__(self, entities, time=None):\n",
    "        self.entities = list(entities)\n",
    "        self.time = time\n",
    "    def perform(self, query):\n",
    "        return [{'entities': self.entities, 'time': self.time}], None\n",
    "\n",
    "def evaluate_sample(sample_data, mode='proposed'):\n",
    "    # Modes: 'baseline' (No Temporal), 'proposed' (Filtering+Sorted), 'pure_tcomplex'\n",
    "    results = []\n",
    "    original_scorer = engine.temporal_scorer\n",
    "    \n",
    "    # Configure Engine\n",
    "    use_temporal_logic = False\n",
    "    if mode == 'baseline':\n",
    "        engine.temporal_scorer = None # Disable TComplex Reranking\n",
    "        use_temporal_logic = False\n",
    "    elif mode == 'proposed':\n",
    "        use_temporal_logic = True\n",
    "    elif mode == 'pure_tcomplex':\n",
    "        pass \n",
    "\n",
    "    for item in tqdm(sample_data, desc=f\"Eval {mode}\"):\n",
    "        q = item['question']\n",
    "        gt_ids = item['entities']\n",
    "        answers = item['answers']\n",
    "        q_type = item['type']\n",
    "        \n",
    "        # --- PURE TCOMPLEX PATH ---\n",
    "        if mode == 'pure_tcomplex':\n",
    "            # Pure Mode: Use CronKGQA model directly\n",
    "            try:\n",
    "                top_preds = engine.get_pure_tcomplex_rank(q, gt_ids, top_k=10)\n",
    "            except Exception as e:\n",
    "                top_preds = []\n",
    "            \n",
    "            rank = 1000\n",
    "            for i, pred_id in enumerate(top_preds):\n",
    "                # Convert QA Model entity/time ID string to set presence check\n",
    "                valid = False\n",
    "                # Check exact string match\n",
    "                if pred_id in answers:\n",
    "                    valid = True\n",
    "                # Check int match (years)\n",
    "                try:\n",
    "                    if str(pred_id).isdigit() and int(pred_id) in answers:\n",
    "                         valid = True\n",
    "                except: pass\n",
    "                \n",
    "                if valid:\n",
    "                    rank = i + 1\n",
    "                    break\n",
    "            \n",
    "            results.append({\n",
    "                'question_type': item['type'],\n",
    "                'rank': rank,\n",
    "                'hit_1': 1 if rank == 1 else 0,\n",
    "                'hit_5': 1 if rank <= 5 else 0,\n",
    "                'mrr': 1.0/rank if rank <= 10 else 0.0\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # --- HYBRID PATH ---\n",
    "        names = [mapper.get_label(qid) for qid in gt_ids]\n",
    "        t_set = item.get('times', set())\n",
    "        if t_set and len(t_set) > 0:\n",
    "            t = list(t_set)[0]\n",
    "        else:\n",
    "            matches = re.findall(r'\\b(1\\d{3}|20\\d{2})\\b', q)\n",
    "            t = int(matches[0]) if matches else None\n",
    "        \n",
    "        engine.extractor_override = MockExtractor(names, t)\n",
    "        \n",
    "        search_k = 50 if (use_temporal_logic and q_type in ['simple_time', 'before_after', 'first_last']) else 10\n",
    "        \n",
    "        try:\n",
    "            ranked = engine.get_ranked_results(q, top_k=search_k)\n",
    "        except Exception as e:\n",
    "            ranked = []\n",
    "            \n",
    "        rank = 1000\n",
    "        logic_applied = False\n",
    "        \n",
    "        if use_temporal_logic:\n",
    "            # 1. simple_time\n",
    "            if q_type == 'simple_time':\n",
    "                for i, res in enumerate(ranked):\n",
    "                    t_obj = res['triplet']\n",
    "                    extracted_year = None\n",
    "                    if t_obj.time and t_obj.time.name: extracted_year = t_obj.time.name\n",
    "                    elif 'time' in t_obj.relation.prop: extracted_year = t_obj.relation.prop['time']\n",
    "                    elif 'time' in t_obj.end_node.prop: extracted_year = t_obj.end_node.prop['time']\n",
    "                    \n",
    "                    if extracted_year:\n",
    "                        try:\n",
    "                            y_str = str(extracted_year).split('-')[0]\n",
    "                            if y_str.isdigit():\n",
    "                                 y_int = int(y_str)\n",
    "                                 if y_int in answers or str(y_int) in answers:\n",
    "                                     rank = i + 1\n",
    "                                     logic_applied = True\n",
    "                                     break\n",
    "                        except: pass\n",
    "\n",
    "            # 2. before_after\n",
    "            elif q_type == 'before_after':\n",
    "                ref_year = None\n",
    "                for res in ranked:\n",
    "                    t_obj = res['triplet']\n",
    "                    y_val = None\n",
    "                    if t_obj.time and t_obj.time.name: y_val = t_obj.time.name\n",
    "                    elif 'time' in t_obj.relation.prop: y_val = t_obj.relation.prop['time']\n",
    "                    \n",
    "                    s_id = t_obj.start_node.prop.get('wd_id')\n",
    "                    o_id = t_obj.end_node.prop.get('wd_id')\n",
    "                    if (s_id in gt_ids or o_id in gt_ids) and y_val:\n",
    "                        try:\n",
    "                            ref_year = int(str(y_val).split('-')[0])\n",
    "                            break \n",
    "                        except: pass\n",
    "                \n",
    "                if ref_year:\n",
    "                    valid_indices = []\n",
    "                    for i, res in enumerate(ranked):\n",
    "                        t_obj = res['triplet']\n",
    "                        y_val = None\n",
    "                        if t_obj.time and t_obj.time.name: y_val = t_obj.time.name\n",
    "                        elif 'time' in t_obj.relation.prop: y_val = t_obj.relation.prop['time']\n",
    "                        elif 'time' in t_obj.end_node.prop: y_val = t_obj.end_node.prop['time'] \n",
    "                        if y_val:\n",
    "                            try:\n",
    "                                cand_y = int(str(y_val).split('-')[0])\n",
    "                                is_before = 'before' in q.lower()\n",
    "                                is_after = 'after' in q.lower()\n",
    "                                if is_before and cand_y < ref_year: valid_indices.append(i)\n",
    "                                elif is_after and cand_y > ref_year: valid_indices.append(i)\n",
    "                            except: pass\n",
    "                    if valid_indices:\n",
    "                        for idx in valid_indices:\n",
    "                            t = ranked[idx]['triplet']\n",
    "                            s = t.start_node.prop.get('wd_id')\n",
    "                            o = t.end_node.prop.get('wd_id')\n",
    "                            if s in answers or o in answers:\n",
    "                                rank = 1 \n",
    "                                logic_applied = True\n",
    "                                break\n",
    "\n",
    "            # 3. first_last\n",
    "            elif q_type == 'first_last':\n",
    "                timed_candidates = []\n",
    "                for i, res in enumerate(ranked):\n",
    "                    t_obj = res['triplet']\n",
    "                    y_val = None\n",
    "                    if t_obj.time and t_obj.time.name: y_val = t_obj.time.name\n",
    "                    elif 'time' in t_obj.relation.prop: y_val = t_obj.relation.prop['time']\n",
    "                    elif 'time' in t_obj.end_node.prop: y_val = t_obj.end_node.prop['time']\n",
    "                    if y_val:\n",
    "                        try:\n",
    "                            y_int = int(str(y_val).split('-')[0])\n",
    "                            timed_candidates.append((i, y_int, res))\n",
    "                        except: pass\n",
    "                \n",
    "                if timed_candidates:\n",
    "                    timed_candidates.sort(key=lambda x: x[1])\n",
    "                    target_idx = -1\n",
    "                    if 'first' in q.lower() or 'initial' in q.lower(): target_idx = 0 \n",
    "                    elif 'last' in q.lower() or 'most recent' in q.lower(): target_idx = -1 \n",
    "                    if target_idx != -1:\n",
    "                        best_cand_tuple = timed_candidates[target_idx] if target_idx < len(timed_candidates) else None\n",
    "                        if best_cand_tuple:\n",
    "                            t = best_cand_tuple[2]['triplet']\n",
    "                            s = t.start_node.prop.get('wd_id')\n",
    "                            o = t.end_node.prop.get('wd_id')\n",
    "                            if s in answers or o in answers:\n",
    "                                rank = 1 \n",
    "                                logic_applied = True\n",
    "\n",
    "        if not logic_applied:\n",
    "            for i, res in enumerate(ranked):\n",
    "                t = res['triplet']\n",
    "                s = t.start_node.prop.get('wd_id')\n",
    "                o = t.end_node.prop.get('wd_id')\n",
    "                if s in answers or o in answers:\n",
    "                    rank = i + 1\n",
    "                    break\n",
    "        \n",
    "        results.append({\n",
    "            'question_type': item['type'],\n",
    "            'rank': rank,\n",
    "            'hit_1': 1 if rank == 1 else 0,\n",
    "            'hit_5': 1 if rank <= 5 else 0,\n",
    "            'mrr': 1.0/rank if rank <= 10 else 0.0\n",
    "        })\n",
    "        \n",
    "    engine.temporal_scorer = original_scorer\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f8df6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Baseline + Finetuning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fac8cc5b0a146499c233aac1038df8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval baseline:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TComplex + Baseline + Finetuning (Proposed)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc23e10f4314850b70993f6a6803c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval proposed:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TComplex (Pure)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c948edafcc4d5995ec3aa32a61e8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval pure_tcomplex:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Run Benchmark\n",
    "SAMPLE_SIZE = 500 \n",
    "subset = test_data[:SAMPLE_SIZE]\n",
    "\n",
    "print(\"Running Baseline + Finetuning...\")\n",
    "df_base = evaluate_sample(subset, mode='baseline')\n",
    "\n",
    "print(\"Running TComplex + Baseline + Finetuning (Proposed)...\")\n",
    "df_prop = evaluate_sample(subset, mode='proposed')\n",
    "\n",
    "print(\"Running TComplex (Pure)...\")\n",
    "df_pure = evaluate_sample(subset, mode='pure_tcomplex')\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7472ab0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Config           Type       MRR    Hits@1  \\\n",
      "0              Baseline + Finetuning        Overall  0.427168  0.372000   \n",
      "2                    TComplex (Pure)        Overall  0.000000  0.000000   \n",
      "1   TComplex + Baseline + Finetuning        Overall  0.564479  0.500000   \n",
      "9              Baseline + Finetuning   before_after  0.231810  0.161290   \n",
      "11                   TComplex (Pure)   before_after  0.000000  0.000000   \n",
      "10  TComplex + Baseline + Finetuning   before_after  0.623387  0.580645   \n",
      "6              Baseline + Finetuning     first_last  0.373344  0.309524   \n",
      "8                    TComplex (Pure)     first_last  0.000000  0.000000   \n",
      "7   TComplex + Baseline + Finetuning     first_last  0.439756  0.398810   \n",
      "15             Baseline + Finetuning  simple_entity  0.892109  0.843972   \n",
      "17                   TComplex (Pure)  simple_entity  0.000000  0.000000   \n",
      "16  TComplex + Baseline + Finetuning  simple_entity  0.903101  0.858156   \n",
      "12             Baseline + Finetuning    simple_time  0.000000  0.000000   \n",
      "14                   TComplex (Pure)    simple_time  0.000000  0.000000   \n",
      "13  TComplex + Baseline + Finetuning    simple_time  0.425335  0.330097   \n",
      "3              Baseline + Finetuning      time_join  0.313840  0.175439   \n",
      "5                    TComplex (Pure)      time_join  0.000000  0.000000   \n",
      "4   TComplex + Baseline + Finetuning      time_join  0.313840  0.175439   \n",
      "\n",
      "      Hits@5  \n",
      "0   0.502000  \n",
      "2   0.000000  \n",
      "1   0.652000  \n",
      "9   0.322581  \n",
      "11  0.000000  \n",
      "10  0.677419  \n",
      "6   0.446429  \n",
      "8   0.000000  \n",
      "7   0.476190  \n",
      "15  0.957447  \n",
      "17  0.000000  \n",
      "16  0.957447  \n",
      "12  0.000000  \n",
      "14  0.000000  \n",
      "13  0.572816  \n",
      "3   0.543860  \n",
      "5   0.000000  \n",
      "4   0.543860  \n"
     ]
    }
   ],
   "source": [
    "# Analysis\n",
    "def compare_metrics(runs):\n",
    "    metrics = []\n",
    "    # Overall\n",
    "    for name, df in runs:\n",
    "        metrics.append({\n",
    "            'Config': name,\n",
    "            'Type': 'Overall',\n",
    "            'MRR': df['mrr'].mean(),\n",
    "            'Hits@1': df['hit_1'].mean(),\n",
    "            'Hits@5': df['hit_5'].mean()\n",
    "        })\n",
    "    # Per Type\n",
    "    types = runs[0][1]['question_type'].unique()\n",
    "    for t in types:\n",
    "        for name, df in runs:\n",
    "            sub = df[df['question_type'] == t]\n",
    "            metrics.append({\n",
    "                'Config': name,\n",
    "                'Type': t,\n",
    "                'MRR': sub['mrr'].mean(),\n",
    "                'Hits@1': sub['hit_1'].mean(),\n",
    "                'Hits@5': sub['hit_5'].mean()\n",
    "            })\n",
    "    return pd.DataFrame(metrics).sort_values(['Type', 'Config'])\n",
    "\n",
    "runs = [\n",
    "    ('Baseline + Finetuning', df_base),\n",
    "    ('TComplex + Baseline + Finetuning', df_prop),\n",
    "    ('TComplex (Pure)', df_pure)\n",
    "]\n",
    "\n",
    "final = compare_metrics(runs)\n",
    "print(final)\n",
    "final.to_csv(\"benchmark_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
