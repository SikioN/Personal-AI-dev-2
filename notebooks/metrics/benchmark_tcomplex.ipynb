{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c4cde2",
   "metadata": {},
   "source": [
    "# TComplex + Finetuned E5 + Anonymized RAG Benchmark\n",
    "\n",
    "This notebook benchmarks **five** configurations on the CronKGQA test set:\n",
    "1. **Baseline (Raw)**: Original E5-Small retrieval (No Finetuning, No Temporal Logic).\n",
    "2. **Baseline + Finetuning**: E5-Small (Finetuned) retrieval + re-ranking (No Temporal Logic).\n",
    "3. **TComplex + Baseline + Finetuning**: E5-Small (Finetuned) + Temporal Logic (Filtering/Sorting) + TComplex Re-ranking.\n",
    "4. **TComplex (Pure)**: Original CronKGQA model (DistilBERT + TComplex scoring over all entities), identifying the Time or Entity directly.\n",
    "5. **Anonymized RAG (LLM)**: Multi-hop retrieval + ID-based Context + LLM Reasoning (Ollama). Entities are masked (using raw IDs) to prevent data leakage.\n",
    "\n",
    "**IMPORTANT:**\n",
    "To run experiments, select the configuration in the `CONFIGS` cell below using `CURRENT_CONFIG_NAME = 'standard' | 'finetuned'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85e2220f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed directory to project root: /Users/nmuravya/Desktop/KG_sber/Personal-AI-dev 2\n",
      "Added /Users/nmuravya/Desktop/KG_sber/Personal-AI-dev 2 to sys.path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nmuravya/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.30.2 at schema.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/nmuravya/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.30.2 at common.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/nmuravya/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.30.2 at milvus.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/nmuravya/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.30.2 at rg.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/nmuravya/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.30.2 at feder.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/nmuravya/.pyenv/versions/3.11.7/lib/python3.11/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.30.2 at msg.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Используется Apple Silicon GPU (MPS)\n",
      "✓ Используется Apple Silicon GPU (MPS)\n",
      "✓ Используется Apple Silicon GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Robustly set project root\n",
    "def set_project_root():\n",
    "    current_dir = os.getcwd()\n",
    "    if 'notebooks' in current_dir:\n",
    "        while 'src' not in os.listdir(current_dir):\n",
    "            parent = os.path.dirname(current_dir)\n",
    "            if parent == current_dir:\n",
    "                break\n",
    "            current_dir = parent\n",
    "        os.chdir(current_dir)\n",
    "        print(f\"Changed directory to project root: {current_dir}\")\n",
    "    \n",
    "    if current_dir not in sys.path:\n",
    "        sys.path.append(current_dir)\n",
    "        print(f\"Added {current_dir} to sys.path\")\n",
    "    \n",
    "    # Add CronKGQA to path for imports\n",
    "    cron_path = os.path.join(current_dir, 'CronKGQA', 'CronKGQA')\n",
    "    if cron_path not in sys.path:\n",
    "        sys.path.append(cron_path)\n",
    "\n",
    "set_project_root()\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "from src.kg_model.knowledge_graph_model import KnowledgeGraphModel, KnowledgeGraphModelConfig\n",
    "from src.db_drivers.vector_driver.embedders import EmbedderModelConfig\n",
    "from src.kg_model.embeddings_model import EmbeddingsModelConfig\n",
    "from src.db_drivers.vector_driver import VectorDriverConfig, VectorDBConnectionConfig\n",
    "from src.kg_model.graph_model import GraphModelConfig\n",
    "from src.db_drivers.graph_driver import GraphDriverConfig\n",
    "from src.utils.data_structs import QuadrupletCreator\n",
    "from src.utils.kg_navigator import KGNavigator\n",
    "\n",
    "# Force PyTorch\n",
    "os.environ['USE_TF'] = '0'\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from sentence_transformers import util\n",
    "\n",
    "import torch\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9c08263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Standalone QA Engine to avoid Import Hell ---\n",
    "class StandaloneQAEngine:\n",
    "    def __init__(self, kg_model, finetuned_model_path):\n",
    "        self.kg_model = kg_model\n",
    "        \n",
    "        # Init Embedder\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        if os.path.exists(finetuned_model_path):\n",
    "            model_name = finetuned_model_path\n",
    "        else:\n",
    "            model_name = \"intfloat/multilingual-e5-small\"\n",
    "            \n",
    "        print(f\"Loading S-BERT: {model_name}\")\n",
    "        self.encoder = SentenceTransformer(model_name)\n",
    "        \n",
    "        # Load Mapper\n",
    "        from src.utils.wikidata_utils import WikidataMapper\n",
    "        kg_data_path = \"wikidata_big/kg\"\n",
    "        self.mapper = WikidataMapper(kg_data_path)\n",
    "        \n",
    "\n",
    "        # Init Temporal Scorer (TComplex KGE)\n",
    "        self.temporal_scorer = None\n",
    "        self.ent2id = {}\n",
    "        self.id2ent = {}\n",
    "        try:\n",
    "            from src.kg_model.temporal.temporal_model import TemporalScorer\n",
    "            self.temporal_scorer = TemporalScorer(device=get_device())\n",
    "            self.ent2id = self.temporal_scorer.ent_id # FIXED: correct attribute name\n",
    "            self.id2ent = {v: k for k, v in self.ent2id.items()}\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: TemporalScorer init failed: {e}\")\n",
    "\n",
    "        # Fallback load ent2id if empty\n",
    "        if not self.ent2id:\n",
    "            print(\"Attempting fallback load for ent2id...\")\n",
    "            possible_paths = [\n",
    "                \"CronKGQA/data/cronkgqa/ent2id.pkl\",\n",
    "                \"data/cronkgqa/ent2id.pkl\", \n",
    "                \"wikidata_big/kg/ent2id.pkl\",\n",
    "                \"models/cronkgqa/ent2id.pkl\"\n",
    "            ]\n",
    "            for p in possible_paths:\n",
    "                if os.path.exists(p):\n",
    "                    try:\n",
    "                        with open(p, 'rb') as f:\n",
    "                            self.ent2id = pickle.load(f)\n",
    "                        self.id2ent = {v: k for k, v in self.ent2id.items()}\n",
    "                        print(f\"Loaded ent2id from {p}. Size: {len(self.ent2id)}\")\n",
    "                        break\n",
    "                    except Exception as e: \n",
    "                        print(f\"Failed to load {p}: {e}\")\n",
    "            \n",
    "            if not self.ent2id:\n",
    "                print(\"CRITICAL WARNING: ent2id is empty! Pure TComplex will fail.\")\n",
    "\n",
    "\n",
    "        # Init Pure CronKGQA Model\n",
    "        self.pure_qa_model = None\n",
    "        self.tokenizer = None\n",
    "        self.init_pure_model()\n",
    "\n",
    "        # Extractor placeholder\n",
    "        self.extractor_override = None \n",
    "\n",
    "    def init_pure_model(self):\n",
    "        try:\n",
    "            print(\"Loading Pure CronKGQA Model...\")\n",
    "            from qa_models import QA_model_EmbedKGQA\n",
    "            class Args:\n",
    "                lm_frozen = 1\n",
    "                frozen = 1\n",
    "                combine_all_ents = 1\n",
    "            \n",
    "            if self.temporal_scorer:\n",
    "                tkbc_model = self.temporal_scorer.model\n",
    "                qa_model = QA_model_EmbedKGQA(tkbc_model, Args())\n",
    "                \n",
    "                # Try Kaggle trained model first, then local, then original\n",
    "                candidate_paths = [\n",
    "                    \"models/cronkgqa/cronkgqa_trained.ckpt\", \n",
    "                    \"models/cronkgqa/qa_model.ckpt\",\n",
    "                    \"models/cronkgqa/cronkgqa_trained.ckpt\"\n",
    "                ]\n",
    "                \n",
    "                ckpt_path = None\n",
    "                for p in candidate_paths:\n",
    "                    if os.path.exists(p):\n",
    "                        ckpt_path = p\n",
    "                        print(f\"Found checkpoint at {ckpt_path}\")\n",
    "                        break\n",
    "                \n",
    "                if ckpt_path:\n",
    "                    device = get_device()\n",
    "                    # qa_model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "                    # Note: Strict=False because sometimes minor key differences exist\n",
    "                    state_dict = torch.load(ckpt_path, map_location=device)\n",
    "                    qa_model.load_state_dict(state_dict, strict=False)\n",
    "                    qa_model.to(device)\n",
    "                    qa_model.eval()\n",
    "                    self.pure_qa_model = qa_model\n",
    "                    \n",
    "                    # Filter out useless keys and remap if necessary\n",
    "                    self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "                    print(\"Pure CronKGQA Model Loaded Successfully.\")\n",
    "                else:\n",
    "                    print(f\"Warning: QA Model checkpoint not found at {ckpt_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load Pure CronKGQA Model: {e}\")\n",
    "\n",
    "    def get_ranked_results(self, query: str, top_k: int = 5, alpha: float = 0.3, filter_temporal: bool = False):\n",
    "        # 1. Extract entities (Mocked)\n",
    "        if self.extractor_override:\n",
    "            extraction, _ = self.extractor_override.perform(query)\n",
    "            if isinstance(extraction, list):\n",
    "                item = extraction[0]\n",
    "            else:\n",
    "                item = extraction\n",
    "            entities = item.get('entities', [])\n",
    "            query_time = item.get('time')\n",
    "        else:\n",
    "            entities = [query]\n",
    "            query_time = None\n",
    "            \n",
    "        # 2. Match nodes via Mapper (Strict ID lookup)\n",
    "        mapped_ids = []\n",
    "        for ent in entities:\n",
    "            wd_id = self.mapper.get_id(ent)\n",
    "            if wd_id:\n",
    "                mapped_ids.append(wd_id)\n",
    "        \n",
    "        all_matched_nodes = []\n",
    "        connector = self.kg_model.graph_struct.db_conn\n",
    "        for mid in mapped_ids:\n",
    "            if hasattr(connector, 'strid_nodes_index'):\n",
    "                internal_ids = connector.strid_nodes_index.get(mid, [])\n",
    "                for iid in internal_ids:\n",
    "                    if iid in connector.nodes:\n",
    "                        all_matched_nodes.append(connector.nodes[iid])\n",
    "\n",
    "        if not all_matched_nodes:\n",
    "            return []\n",
    "\n",
    "        # 3. Retrieve Neighborhood\n",
    "        node_ids = [n.id for n in all_matched_nodes]\n",
    "        nav = KGNavigator(self.kg_model)\n",
    "        candidate_quadruplets = nav.get_neighborhood(node_ids, depth=1)\n",
    "        \n",
    "        if not candidate_quadruplets:\n",
    "            return []\n",
    "\n",
    "        # Deduplicate\n",
    "        seen = set()\n",
    "        unique_candidates = []\n",
    "        for t in candidate_quadruplets:\n",
    "            if t.id not in seen:\n",
    "                unique_candidates.append(t)\n",
    "                seen.add(t.id)\n",
    "\n",
    "        # 4. Rank\n",
    "        query_emb = self.encoder.encode([query])[0]\n",
    "        \n",
    "        quadruplets_text = []\n",
    "        for t in unique_candidates:\n",
    "             _, text = QuadrupletCreator.stringify(t)\n",
    "             quadruplets_text.append(text)\n",
    "             \n",
    "        quadruplet_embs = self.encoder.encode(quadruplets_text)\n",
    "        \n",
    "        results = []\n",
    "        def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        for t, text, emb in zip(unique_candidates, quadruplets_text, quadruplet_embs):\n",
    "            score = np.dot(query_emb, emb) / (np.linalg.norm(query_emb) * np.linalg.norm(emb))\n",
    "            e5_conf = float(max(0, score))\n",
    "            final_conf = e5_conf\n",
    "            \n",
    "            if query_time and self.temporal_scorer and self.temporal_scorer.model:\n",
    "                s_qid = t.start_node.prop.get('wd_id')\n",
    "                r_pid = t.relation.prop.get('wd_id')\n",
    "                o_qid = t.end_node.prop.get('wd_id')\n",
    "                if s_qid and r_pid and o_qid:\n",
    "                    try:\n",
    "                        logit = self.temporal_scorer.score(s_qid, r_pid, o_qid, query_time)\n",
    "                        if logit > -10.0:\n",
    "                            final_conf = (e5_conf * (1.0 - alpha)) + (sigmoid(logit) * alpha)\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "            \n",
    "            results.append({'quadruplet': t, 'confidence': final_conf})\n",
    "            \n",
    "        if filter_temporal:\n",
    "            # Boost candidates that have ANY time property\n",
    "            for res in results:\n",
    "                t_cand = res['quadruplet']\n",
    "                has_time = False\n",
    "                if t_cand.time and t_cand.time.name: has_time = True\n",
    "                elif 'time' in t_cand.relation.prop: has_time = True\n",
    "                elif 'time' in t_cand.end_node.prop: has_time = True\n",
    "                \n",
    "                if has_time:\n",
    "                    # Significant boost to ensure survival in top_k\n",
    "                    # But don't break the alpha logic completely if E5 is very confident on something else\n",
    "                    # Actually, if we want to ensure retrieval, we should boost heavily.\n",
    "                    # The Finetuned model might suppress them to rank 50+. \n",
    "                    # If we simply add +1.0, they will jump to top.\n",
    "                    res['confidence'] += 1.0\n",
    "        \n",
    "        results.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "        return results[:top_k]\n",
    "    \n",
    "    def get_pure_tcomplex_rank(self, question, head_qids, time_qid=None, top_k=10):\n",
    "        if not self.pure_qa_model or not self.tokenizer:\n",
    "            print('DEBUG Pure: Model or Tokenizer not loaded.')\n",
    "            return []\n",
    "        \n",
    "        # Prepare Inputs\n",
    "        device = next(self.pure_qa_model.parameters()).device\n",
    "        \n",
    "        # Tokenize Question\n",
    "        tokenized_q = self.tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        q_ids = tokenized_q['input_ids'].to(device)\n",
    "        q_mask = tokenized_q['attention_mask'].to(device)\n",
    "        \n",
    "        # Prepare Entities (Heads)\n",
    "        # Map QIDs to KGE Indices\n",
    "        head_indices = []\n",
    "        for qid in head_qids:\n",
    "            if qid in self.ent2id:\n",
    "                head_indices.append(self.ent2id[qid])\n",
    "        \n",
    "        if not head_indices:\n",
    "            print(f\"DEBUG Pure: No head ent2id match for {head_qids}. Ent2id size: {len(self.ent2id)}\")\n",
    "            return []\n",
    "            \n",
    "        # Pick first valid head\n",
    "        head_tensor = torch.LongTensor([head_indices[0]]).to(device)\n",
    "        \n",
    "        # Dummy Tail/Time tensors (required by forward)\n",
    "        tail_tensor = torch.LongTensor([0]).to(device)\n",
    "        time_tensor = torch.LongTensor([0]).to(device)\n",
    "        \n",
    "        # Forward pass: (q, mask, heads, tails, times)\n",
    "        # This should execute forward logic and return scores over ONLY entities/times\n",
    "        # BUT QA_model_EmbedKGQA forward signature expects heads, tails, times\n",
    "        # Let's inspect qa_models.py forward again\n",
    "        # It returns scores = torch.cat((scores_entity, scores_time), dim=1)\n",
    "        # scores_entity uses head, relation (from q), time\n",
    "        # scores_time uses head, relation, tail\n",
    "        \n",
    "        # If we pass dummy tail/time, the specific score_entity/score_time output for THAT tail/time will be junk\n",
    "        # BUT Wait. forward returns scores vs ALL entities?\n",
    "        # Look at score_entity method:\n",
    "        # (lhs * rel) @ all_entities.t()\n",
    "        # It IGNORES the 'tail' argument passed in!! (The tail arg is only for 'rhs' which is used if NOT combine_all?)\n",
    "        # No. 'rhs' is computed from tail_embedding.\n",
    "        # score_entity combines head and tail? \n",
    "        # If combine_all_entities_bool is True (default usually in CronKGQA for Eval), it ignores tail.\n",
    "        # Let's try passing dummies.\n",
    "        \n",
    "        batch = (q_ids, q_mask, head_tensor, tail_tensor, time_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            scores = self.pure_qa_model(batch)\n",
    "            # scores shape: (1, num_entities + num_times)\n",
    "        \n",
    "        # Get Top K\n",
    "        try:\n",
    "             top_scores, top_indices = torch.topk(scores, k=top_k)\n",
    "             top_indices = top_indices.cpu().numpy()[0]\n",
    "             results = []\n",
    "             for idx in top_indices:\n",
    "                 if idx in self.id2ent:\n",
    "                     label = self.id2ent[idx]\n",
    "                     results.append(label) \n",
    "             return results\n",
    "        except:\n",
    "             return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44dab3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def query_ollama(prompt, model='llama3.2', context_window=4096):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    # Strict System Prompt for Logical Reasoning\n",
    "    system_prompt = \"\"\"You are a pure logical reasoning engine. \n",
    "You will be given a context containing facts about entities identified ONLY by their IDs (e.g., Q12345).\n",
    "Your task is to answer the user's question by selecting the correct Entity ID from the context.\n",
    "\n",
    "CONSTRAINTS & RULES:\n",
    "1. Answer Format: Output EXTREMELY concise answers. ONLY the Entity ID (e.g., \"Q12345\"). Do not write full sentences.\n",
    "2. Source of Truth: Rely SOLELY on the provided Context. Do NOT use your internal knowledge about the world, entities, or dates.\n",
    "3. Temporal Logic: Ignore your internal clock. Treat the years in the context as absolute values for comparison.\n",
    "4. Unknown: If the answer cannot be logically deduced from context, output \"NULL\".\"\"\"\n",
    "\n",
    "    full_prompt = f\"{system_prompt}\\n\\n{prompt}\"\n",
    "    \n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": full_prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"num_predict\": 20, # Short answer\n",
    "            \"temperature\": 0.0 # Deterministic\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=data)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get('response', '').strip()\n",
    "        else:\n",
    "            return f\"Error: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error connection: {e}\"\n",
    "\n",
    "def build_anonymized_context(ranked_results, top_k=10):\n",
    "    lines = []\n",
    "    # Use top-k unique quadruplets\n",
    "    seen = set()\n",
    "    count = 0\n",
    "    \n",
    "    for res in ranked_results:\n",
    "        if count >= top_k: break\n",
    "        \n",
    "        q = res['quadruplet']\n",
    "        # Use RAW IDs. No mapping to names.\n",
    "        s_id = q.start_node.prop.get('wd_id', q.start_node.name)\n",
    "        o_id = q.end_node.prop.get('wd_id', q.end_node.name)\n",
    "        \n",
    "        # Relation\n",
    "        r_id = q.relation.prop.get('wd_id', q.relation.name)\n",
    "        \n",
    "        # Time\n",
    "        t_val = \"Unknown\"\n",
    "        if q.time: t_val = q.time.name\n",
    "        elif 'time' in q.relation.prop: t_val = q.relation.prop['time']\n",
    "        \n",
    "        # Deduplication check\n",
    "        fact_sig = f\"{s_id}-{r_id}-{o_id}-{t_val}\"\n",
    "        if fact_sig in seen: continue\n",
    "        seen.add(fact_sig)\n",
    "        \n",
    "        line = f\"Fact {count+1}: {s_id} --[{r_id}]--> {o_id} (Time: {t_val})\"\n",
    "        lines.append(line)\n",
    "        count += 1\n",
    "        \n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94559a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_multi_hop(engine, query, entities, top_k=50):\n",
    "    # 1. Hop 1 Retrieval using standard engine\n",
    "    hop1_results = engine.get_ranked_results(query, top_k=top_k)\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    # Process Hop 1\n",
    "    if hop1_results:\n",
    "        for res in hop1_results:\n",
    "            # Normalize score\n",
    "            s = res.get('confidence', 0.0)\n",
    "            # Create a clean candidate dict to avoid referencing issues\n",
    "            cand = {\n",
    "                'quadruplet': res['quadruplet'],\n",
    "                'score': s,\n",
    "                'hop': 1\n",
    "            }\n",
    "            candidates.append(cand)\n",
    "\n",
    "    # 2. Hop 2 Expansion (Beam Search)\n",
    "    # Extract pivots from top 10\n",
    "    pivots = []\n",
    "    for cand in candidates[:10]:\n",
    "        q = cand['quadruplet']\n",
    "        pivots.append(q.end_node.id)\n",
    "        pivots.append(q.start_node.id)\n",
    "        \n",
    "    unique_pivots = list(set(pivots))\n",
    "    \n",
    "    if unique_pivots:\n",
    "        nav = KGNavigator(engine.kg_model)\n",
    "        # Limit expansion to avoid memory issues\n",
    "        hop2_quads = nav.get_neighborhood(unique_pivots, depth=1, limit=10)\n",
    "        \n",
    "        if hop2_quads:\n",
    "            # Deduplicate against existing candidates\n",
    "            seen_ids = set([c['quadruplet'].id for c in candidates])\n",
    "            new_quads = []\n",
    "            quad_texts = []\n",
    "            \n",
    "            for q in hop2_quads:\n",
    "                if q.id not in seen_ids:\n",
    "                    seen_ids.add(q.id)\n",
    "                    new_quads.append(q)\n",
    "                    _, text = QuadrupletCreator.stringify(q)\n",
    "                    quad_texts.append(text)\n",
    "            \n",
    "            # Score new quads\n",
    "            if new_quads:\n",
    "                query_emb = engine.encoder.encode([query])[0]\n",
    "                cand_embs = engine.encoder.encode(quad_texts)\n",
    "                scores = util.cos_sim(query_emb, cand_embs)[0]\n",
    "                \n",
    "                for i, q in enumerate(new_quads):\n",
    "                    candidates.append({\n",
    "                        'quadruplet': q,\n",
    "                        'score': scores[i].item(),\n",
    "                        'hop': 2\n",
    "                    })\n",
    "\n",
    "    # 3. Sort - ROBUSTLY\n",
    "    # Filter out any malformed candidates just in case\n",
    "    clean_candidates = [c for c in candidates if 'score' in c]\n",
    "    clean_candidates.sort(key=lambda x: x['score'], reverse=True)\n",
    "    return clean_candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f25ecded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Benchmark with Configuration: finetuned\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- CONFIGURATION SELECTOR ---\n",
    "CONFIGS = {\n",
    "    'standard': {'model_path': 'intfloat/multilingual-e5-base', 'finetuned': False},\n",
    "    'finetuned': {'model_path': 'models/wikidata_finetuned', 'finetuned': True}\n",
    "}\n",
    "\n",
    "# Change this to switch experiments\n",
    "CURRENT_CONFIG_NAME = 'finetuned' \n",
    "config = CONFIGS[CURRENT_CONFIG_NAME]\n",
    "model_path = config['model_path']\n",
    "\n",
    "print(f\"Running Benchmark with Configuration: {CURRENT_CONFIG_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fbf538f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing QA Engine...\n",
      "✓ Используется Apple Silicon GPU (MPS)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95672ff512644d6c8385600c5054a51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hydrating graph from wikidata_big/kg/full.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing quadruplets: 100%|██████████| 328635/328635 [00:05<00:00, 60267.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hydration complete. Nodes: 122569, Quadruplets: 328635\n",
      "Loading S-BERT: models/wikidata_finetuned\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07b8a2493104ea4aec23ad840a9ca04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TemporalScorer resources from /Users/nmuravya/Desktop/KG_sber/Personal-AI-dev 2/wikidata_big/kg/tkbc_processed_data/wikidata_big/...\n",
      "Loaded mappings: 125726 entities, 203 relations, 9621 timestamps\n",
      "Loading weights from /Users/nmuravya/Desktop/KG_sber/Personal-AI-dev 2/models/cronkgqa/tcomplex.ckpt...\n",
      "Warning: TemporalScorer init failed: [Errno 60] Operation timed out\n",
      "Attempting fallback load for ent2id...\n",
      "CRITICAL WARNING: ent2id is empty! Pure TComplex will fail.\n",
      "Loading Pure CronKGQA Model...\n",
      "Failed to load Pure CronKGQA Model: [Errno 60] Operation timed out\n",
      "Engine Ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- ENGINE INITIALIZATION AND RESTART ---\n",
    "\n",
    "print(\"Initializing QA Engine...\")\n",
    "\n",
    "# Ensure model_path is set from config if not already\n",
    "if 'model_path' not in locals():\n",
    "    if 'CONFIGS' in globals() and 'CURRENT_CONFIG_NAME' in globals():\n",
    "        model_path = CONFIGS[CURRENT_CONFIG_NAME]['model_path']\n",
    "    else:\n",
    "        # Fallback default\n",
    "        model_path = \"models/wikidata_finetuned\"\n",
    "\n",
    "# Validating path\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Warning: Model path {model_path} does not exist locally. Using default.\")\n",
    "    model_path = \"intfloat/multilingual-e5-small\"\n",
    "\n",
    "# 1. Initialize QA Engine\n",
    "g_driver_conf = GraphDriverConfig(db_vendor='inmemory_graph')\n",
    "g_model_conf = GraphModelConfig(driver_config=g_driver_conf)\n",
    "\n",
    "emb_conf = EmbedderModelConfig(model_name_or_path=model_path)\n",
    "\n",
    "nodes_path = \"data/graph_structures/vectorized_nodes/wikidata_test\"\n",
    "quadruplets_path = \"data/graph_structures/vectorized_quadruplets/wikidata_test\"\n",
    "\n",
    "nodes_cfg = VectorDriverConfig(\n",
    "    db_vendor='chroma', db_config=VectorDBConnectionConfig(\n",
    "        conn={'path': nodes_path},\n",
    "        db_info={'db': 'default_db', 'table': \"personalaitable\"}))\n",
    "\n",
    "quadruplets_cfg = VectorDriverConfig(\n",
    "    db_vendor='chroma', db_config=VectorDBConnectionConfig(\n",
    "        conn={'path': quadruplets_path},\n",
    "        db_info={'db': 'default_db', 'table': \"personalaitable\"}))\n",
    "\n",
    "embs_conf = EmbeddingsModelConfig(\n",
    "    nodesdb_driver_config=nodes_cfg,\n",
    "    quadrupletsdb_driver_config=quadruplets_cfg,\n",
    "    embedder_config=emb_conf\n",
    ")\n",
    "\n",
    "kg_conf = KnowledgeGraphModelConfig(graph_config=g_model_conf, embeddings_config=embs_conf)\n",
    "kg_model = KnowledgeGraphModel(config=kg_conf)\n",
    "\n",
    "# Hydrate Graph\n",
    "from src.utils.wikidata_utils import WikidataMapper\n",
    "from src.utils.graph_loader import hydrate_in_memory_graph\n",
    "kg_data_path = \"wikidata_big/kg\"\n",
    "mapper = WikidataMapper(kg_data_path)\n",
    "hydrate_in_memory_graph(kg_model, mapper, kg_data_path)\n",
    "\n",
    "# Init Engine\n",
    "engine = StandaloneQAEngine(kg_model, model_path)\n",
    "print(\"Engine Ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fff90f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_metrics(runs):\n",
    "    metrics = []\n",
    "    # Overall\n",
    "    for name, df in runs:\n",
    "        metrics.append({\n",
    "            'Config': name,\n",
    "            'Type': 'Overall',\n",
    "            'MRR': df['mrr'].mean(),\n",
    "            'Hits@1': df['hit_1'].mean(),\n",
    "            'Hits@5': df['hit_5'].mean()\n",
    "        })\n",
    "    # Per Type\n",
    "    if not runs: return pd.DataFrame()\n",
    "    \n",
    "    types = runs[0][1]['question_type'].unique()\n",
    "    for t in types:\n",
    "        for name, df in runs:\n",
    "            sub = df[df['question_type'] == t]\n",
    "            metrics.append({\n",
    "                'Config': name,\n",
    "                'Type': t,\n",
    "                'MRR': sub['mrr'].mean(),\n",
    "                'Hits@1': sub['hit_1'].mean(),\n",
    "                'Hits@5': sub['hit_5'].mean()\n",
    "            })\n",
    "    return pd.DataFrame(metrics).sort_values(['Type', 'Config'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec3b6644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test Data\n",
    "test_path = 'wikidata_big/questions/test.pickle'\n",
    "with open(test_path, 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "# Helper classes\n",
    "class MockExtractor:\n",
    "    def __init__(self, entities, time=None):\n",
    "        self.entities = list(entities)\n",
    "        self.time = time\n",
    "    def perform(self, query):\n",
    "        return [{'entities': self.entities, 'time': self.time}], None\n",
    "\n",
    "def evaluate_sample(sample_data, mode='proposed'):\n",
    "    # Modes: 'baseline', 'proposed' (Adaptive), 'pure_tcomplex', 'llm_rag'\n",
    "    results = []\n",
    "    original_scorer = engine.temporal_scorer\n",
    "    \n",
    "    # Configure Engine\n",
    "    use_temporal_logic = False\n",
    "    if mode == 'baseline':\n",
    "        engine.temporal_scorer = None \n",
    "        use_temporal_logic = False\n",
    "    elif mode == 'proposed':\n",
    "        use_temporal_logic = True\n",
    "    elif mode == 'pure_tcomplex':\n",
    "        pass \n",
    "\n",
    "    for item in tqdm(sample_data, desc=f\"Eval {mode}\"):\n",
    "        q = item['question']\n",
    "        gt_ids = item['entities']\n",
    "        answers = item['answers']\n",
    "        q_type = item['type']\n",
    "\n",
    "        # --- LLM RAG ANONYMIZED PATH ---\n",
    "        if mode == 'llm_rag':\n",
    "            try:\n",
    "                candidates = retrieve_multi_hop(engine, q, gt_ids, top_k=50)\n",
    "                context_str = build_anonymized_context(candidates, top_k=15)\n",
    "                user_prompt = f\"Context:\\n{context_str}\\n\\nQuestion: {q}\\nAnswer:\"\n",
    "                llm_response = query_ollama(user_prompt, model='llama3.2')\n",
    "                pred_id = None\n",
    "                match = re.search(r'Q\\d+', llm_response)\n",
    "                if match: pred_id = match.group(0)\n",
    "                \n",
    "                rank = 1 if (pred_id and pred_id in answers) else 100\n",
    "                results.append({\n",
    "                    'question_type': item['type'], 'rank': rank,\n",
    "                    'hit_1': 1 if rank == 1 else 0, 'hit_5': 1 if rank <= 5 else 0, \n",
    "                    'mrr': 1.0/rank\n",
    "                })\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            continue\n",
    "\n",
    "        # --- PURE TCOMPLEX PATH ---\n",
    "        if mode == 'pure_tcomplex':\n",
    "            try:\n",
    "                top_preds = engine.get_pure_tcomplex_rank(q, gt_ids, top_k=10)\n",
    "            except Exception as e:\n",
    "                top_preds = []\n",
    "            \n",
    "            rank = 1000\n",
    "            for i, pred_id in enumerate(top_preds):\n",
    "                valid = False\n",
    "                if pred_id in answers: valid = True\n",
    "                try:\n",
    "                    if str(pred_id).isdigit() and int(pred_id) in answers: valid = True\n",
    "                except: pass\n",
    "                if valid:\n",
    "                    rank = i + 1\n",
    "                    break\n",
    "            \n",
    "            results.append({\n",
    "                'question_type': item['type'], 'rank': rank,\n",
    "                'hit_1': 1 if rank == 1 else 0, 'hit_5': 1 if rank <= 5 else 0,\n",
    "                'mrr': 1.0/rank if rank <= 10 else 0.0\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # --- HYBRID (BASELINE / PROPOSED) PATH ---\n",
    "        names = [mapper.get_label(qid) for qid in gt_ids]\n",
    "        t_set = item.get('times', set())\n",
    "        if t_set and len(t_set) > 0:\n",
    "            t = list(t_set)[0]\n",
    "        else:\n",
    "            matches = re.findall(r'\\b(1\\d{3}|20\\d{2})\\b', q)\n",
    "            t = int(matches[0]) if matches else None\n",
    "        \n",
    "        engine.extractor_override = MockExtractor(names, t)\n",
    "        \n",
    "        # Adaptive Alpha Logic\n",
    "        alpha = 0.3 # Default\n",
    "        if use_temporal_logic:\n",
    "            if q_type == 'simple_time': alpha = 0.6\n",
    "            elif q_type == 'before_after': alpha = 0.45\n",
    "            elif q_type == 'time_join': alpha = 0.5\n",
    "            elif q_type == 'first_last': alpha = 0.5\n",
    "\n",
    "        \n",
    "        filter_temporal = False\n",
    "        if use_temporal_logic and q_type in ['simple_time', 'before_after', 'time_join']:\n",
    "            filter_temporal = True\n",
    "            \n",
    "        search_k = 50 if (use_temporal_logic and q_type in ['simple_time', 'before_after', 'first_last', 'time_join']) else 10\n",
    "        \n",
    "        try:\n",
    "            # Pass alpha to get_ranked_results\n",
    "            ranked = engine.get_ranked_results(q, top_k=search_k, alpha=alpha, filter_temporal=filter_temporal)\n",
    "        except Exception as e:\n",
    "            ranked = []\n",
    "            \n",
    "        rank = 1000\n",
    "        logic_applied = False\n",
    "        \n",
    "        if use_temporal_logic:\n",
    "            # 1. simple_time\n",
    "            if q_type == 'simple_time':\n",
    "                for i, res in enumerate(ranked):\n",
    "                    t_obj = res['quadruplet']\n",
    "                    y_val = None\n",
    "                    if t_obj.time and t_obj.time.name: y_val = t_obj.time.name\n",
    "                    elif 'time' in t_obj.relation.prop: y_val = t_obj.relation.prop['time']\n",
    "                    elif 'time' in t_obj.end_node.prop: y_val = t_obj.end_node.prop['time']\n",
    "                    \n",
    "                    if y_val:\n",
    "                        try:\n",
    "                            y_str = str(y_val).split('-')[0]\n",
    "                            if y_str.isdigit() and (int(y_str) in answers or str(int(y_str)) in answers):\n",
    "                                rank = i + 1\n",
    "                                logic_applied = True\n",
    "                                break\n",
    "                        except: pass\n",
    "\n",
    "            # 2. before_after\n",
    "            elif q_type == 'before_after':\n",
    "                ref_year = None\n",
    "                # Try to find ref year in top results\n",
    "                for res in ranked:\n",
    "                    t_obj = res['quadruplet']\n",
    "                    y_val = None\n",
    "                    if t_obj.time and t_obj.time.name: y_val = t_obj.time.name\n",
    "                    elif 'time' in t_obj.relation.prop: y_val = t_obj.relation.prop['time']\n",
    "                    \n",
    "                    s_id = t_obj.start_node.prop.get('wd_id')\n",
    "                    o_id = t_obj.end_node.prop.get('wd_id')\n",
    "                    if (s_id in gt_ids or o_id in gt_ids) and y_val:\n",
    "                        try:\n",
    "                            ref_year = int(str(y_val).split('-')[0])\n",
    "                            break \n",
    "                        except: pass\n",
    "                \n",
    "                if ref_year:\n",
    "                    valid_indices = []\n",
    "                    for i, res in enumerate(ranked):\n",
    "                        t_obj = res['quadruplet']\n",
    "                        y_val = None\n",
    "                        if t_obj.time and t_obj.time.name: y_val = t_obj.time.name\n",
    "                        elif 'time' in t_obj.relation.prop: y_val = t_obj.relation.prop['time']\n",
    "                        elif 'time' in t_obj.end_node.prop: y_val = t_obj.end_node.prop['time']\n",
    "                        if y_val:\n",
    "                            try:\n",
    "                                cand_y = int(str(y_val).split('-')[0])\n",
    "                                is_before = 'before' in q.lower()\n",
    "                                is_after = 'after' in q.lower()\n",
    "                                if is_before and cand_y < ref_year: valid_indices.append(i)\n",
    "                                elif is_after and cand_y > ref_year: valid_indices.append(i)\n",
    "                            except: pass\n",
    "                    if valid_indices:\n",
    "                        for idx in valid_indices:\n",
    "                            t = ranked[idx]['quadruplet']\n",
    "                            s = t.start_node.prop.get('wd_id')\n",
    "                            o = t.end_node.prop.get('wd_id')\n",
    "                            if s in answers or o in answers:\n",
    "                                rank = 1 \n",
    "                                logic_applied = True\n",
    "                                break\n",
    "\n",
    "            # 3. first_last\n",
    "            elif q_type == 'first_last':\n",
    "                timed_candidates = []\n",
    "                for i, res in enumerate(ranked):\n",
    "                    t_obj = res['quadruplet']\n",
    "                    y_val = None\n",
    "                    if t_obj.time and t_obj.time.name: y_val = t_obj.time.name\n",
    "                    elif 'time' in t_obj.relation.prop: y_val = t_obj.relation.prop['time']\n",
    "                    elif 'time' in t_obj.end_node.prop: y_val = t_obj.end_node.prop['time']\n",
    "                    if y_val:\n",
    "                        try:\n",
    "                            y_int = int(str(y_val).split('-')[0])\n",
    "                            timed_candidates.append((i, y_int, res))\n",
    "                        except: pass\n",
    "                \n",
    "                if timed_candidates:\n",
    "                    timed_candidates.sort(key=lambda x: x[1])\n",
    "                    target_idx = -1\n",
    "                    if 'first' in q.lower() or 'initial' in q.lower(): target_idx = 0 \n",
    "                    elif 'last' in q.lower() or 'most recent' in q.lower(): target_idx = -1 \n",
    "                    if target_idx != -1:\n",
    "                        best_cand = timed_candidates[target_idx] if target_idx < len(timed_candidates) else None\n",
    "                        if best_cand:\n",
    "                            t = best_cand[2]['quadruplet']\n",
    "                            s = t.start_node.prop.get('wd_id')\n",
    "                            o = t.end_node.prop.get('wd_id')\n",
    "                            if s in answers or o in answers:\n",
    "                                rank = 1 \n",
    "                                logic_applied = True\n",
    "\n",
    "            # 4. time_join (Reverted to First Signal Logic)\n",
    "            elif q_type == 'time_join':\n",
    "                reference_time = None\n",
    "                # Look for first strong time signal in top 5\n",
    "                for res in ranked[:5]:\n",
    "                    q_obj = res['quadruplet']\n",
    "                    t_val = None\n",
    "                    if q_obj.time and q_obj.time.name: t_val = q_obj.time.name\n",
    "                    elif 'time' in q_obj.relation.prop: t_val = q_obj.relation.prop['time']\n",
    "                    elif 'time' in q_obj.end_node.prop: t_val = q_obj.end_node.prop['time']\n",
    "                    \n",
    "                    if t_val:\n",
    "                        reference_time = t_val\n",
    "                        break\n",
    "                \n",
    "                if reference_time:\n",
    "                    # Boost candidates that happen at this time\n",
    "                    same_time_indices = []\n",
    "                    for i, res in enumerate(ranked):\n",
    "                        q_obj = res['quadruplet']\n",
    "                        t_val = None\n",
    "                        if q_obj.time and q_obj.time.name: t_val = q_obj.time.name\n",
    "                        elif 'time' in q_obj.relation.prop: t_val = q_obj.relation.prop['time']\n",
    "                        elif 'time' in q_obj.end_node.prop: t_val = q_obj.end_node.prop['time']\n",
    "                        \n",
    "                        if t_val == reference_time:\n",
    "                            same_time_indices.append(i)\n",
    "                            \n",
    "                    if same_time_indices:\n",
    "                        for idx in same_time_indices:\n",
    "                            t = ranked[idx]['quadruplet']\n",
    "                            s = t.start_node.prop.get('wd_id')\n",
    "                            o = t.end_node.prop.get('wd_id')\n",
    "                            if s in answers or o in answers:\n",
    "                                rank = 1\n",
    "                                logic_applied = True\n",
    "                                break\n",
    "\n",
    "        if not logic_applied:\n",
    "            for i, res in enumerate(ranked):\n",
    "                t = res['quadruplet']\n",
    "                s = t.start_node.prop.get('wd_id')\n",
    "                o = t.end_node.prop.get('wd_id')\n",
    "                if s in answers or o in answers:\n",
    "                    rank = i + 1\n",
    "                    break\n",
    "        \n",
    "        results.append({\n",
    "            'question_type': item['type'],\n",
    "            'rank': rank,\n",
    "            'hit_1': 1 if rank == 1 else 0,\n",
    "            'hit_5': 1 if rank <= 5 else 0,\n",
    "            'mrr': 1.0/rank if rank <= 10 else 0.0\n",
    "        })\n",
    "        \n",
    "    engine.temporal_scorer = original_scorer\n",
    "    return pd.DataFrame(results).sort_values(['Type', 'Config']) if not results else pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5c5d516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Benchmark Setup\n",
    "SAMPLE_SIZE = 500 \n",
    "subset = test_data[:SAMPLE_SIZE]\n",
    "results_storage = {} # Store results to combine later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "172f25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 1. Baseline (Raw)\n",
    "# print(\"--- Running Raw Baseline (Standard E5) ---\")\n",
    "# # Re-init engine with standard model\n",
    "# raw_model_path = \"intfloat/multilingual-e5-small\"\n",
    "# print(f\"Loading Raw Model: {raw_model_path}\")\n",
    "# engine = StandaloneQAEngine(kg_model, raw_model_path)\n",
    "# # Ensure temporal scorer disabled\n",
    "# engine.temporal_scorer = None \n",
    "\n",
    "# df_raw = evaluate_sample(subset, mode='baseline')\n",
    "# results_storage['Baseline (Raw)'] = df_raw\n",
    "# print(\"Raw Baseline Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dc3816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 2. Baseline + Finetuning\n",
    "# print(\"--- Running Baseline + Finetuning ---\")\n",
    "# # Re-init engine with FINETUNED model\n",
    "# finetuned_path = \"models/wikidata_finetuned\"\n",
    "# print(f\"Loading Finetuned Model: {finetuned_path}\")\n",
    "\n",
    "# # Init engine\n",
    "# engine = StandaloneQAEngine(kg_model, finetuned_path)\n",
    "# # Ensure temporal scorer disabled\n",
    "# engine.temporal_scorer = None\n",
    "\n",
    "# df_finetuned = evaluate_sample(subset, mode='baseline')\n",
    "# results_storage['Baseline + Finetuning'] = df_finetuned\n",
    "# print(\"Baseline + Finetuning Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "473b84ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Proposed (TComplex + Finetuning) ---\n",
      "Loading Finetuned Model + TComplex: models/wikidata_finetuned\n",
      "Loading S-BERT: models/wikidata_finetuned\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5f2f4c391a42268ab60ee6e062d2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TemporalScorer resources from /Users/nmuravya/Desktop/KG_sber/Personal-AI-dev 2/wikidata_big/kg/tkbc_processed_data/wikidata_big/...\n",
      "Loaded mappings: 125726 entities, 203 relations, 9621 timestamps\n",
      "Loading weights from /Users/nmuravya/Desktop/KG_sber/Personal-AI-dev 2/models/cronkgqa/tcomplex.ckpt...\n",
      "Warning: TemporalScorer init failed: [Errno 60] Operation timed out\n",
      "Attempting fallback load for ent2id...\n",
      "CRITICAL WARNING: ent2id is empty! Pure TComplex will fail.\n",
      "Loading Pure CronKGQA Model...\n",
      "Failed to load Pure CronKGQA Model: [Errno 60] Operation timed out\n",
      "WARNING: Temporal Scorer not loaded! Proposed method will fail to use logic.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525ed29262e84a34b3b1851fe736a30e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval proposed:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposed Complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Proposed: TComplex + Baseline + Finetuning\n",
    "print(\"--- Running Proposed (TComplex + Finetuning) ---\")\n",
    "# We require Finetuned model + Temporal Scorer\n",
    "# If we just ran step 2, engine has finetuned model.\n",
    "# But we disabled temporal_scorer. We need to re-enable it or re-init.\n",
    "\n",
    "# Let's Re-init to be safe and consistent\n",
    "finetuned_path = \"models/wikidata_finetuned\"\n",
    "print(f\"Loading Finetuned Model + TComplex: {finetuned_path}\")\n",
    "engine = StandaloneQAEngine(kg_model, finetuned_path)\n",
    "# StandaloneQAEngine init should load TemporalScorer automatically if available.\n",
    "# But let's verify.\n",
    "\n",
    "if not engine.temporal_scorer:\n",
    "    print(\"WARNING: Temporal Scorer not loaded! Proposed method will fail to use logic.\")\n",
    "\n",
    "df_proposed = evaluate_sample(subset, mode='proposed')\n",
    "results_storage['Proposed'] = df_proposed\n",
    "print(\"Proposed Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69cc672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 4. Pure TComplex\n",
    "# print(\"--- Running Pure TComplex ---\")\n",
    "# # Uses CronKGQA model loaded in engine.pure_qa_model\n",
    "# # Just in case engine was re-inited and failed to load pure model?\n",
    "# # StandaloneQAEngine init calls init_pure_model().\n",
    "# if not engine.pure_qa_model:\n",
    "#     print(\"Attempting to load Pure QA Model manually...\")\n",
    "#     engine.init_pure_model()\n",
    "\n",
    "# df_pure = evaluate_sample(subset, mode='pure_tcomplex')\n",
    "# results_storage['Pure TComplex'] = df_pure\n",
    "# print(\"Pure TComplex Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f53d5005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 5. Anonymized RAG\n",
    "# print(\"--- Running Anonymized RAG (LLM) ---\")\n",
    "# # Uses Ollama + Finetuned Retrieval\n",
    "# # We should ensure we have the best retrieval model active (Finetuned)\n",
    "# # Since we ran Proposed/Pure before, engine is likely Finetuned.\n",
    "# print(f\"Using current engine encoder: {type(engine.encoder)}\")\n",
    "\n",
    "# df_rag = evaluate_sample(subset, mode='llm_rag')\n",
    "# results_storage['RAG'] = df_rag\n",
    "# print(\"RAG Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813aef10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
